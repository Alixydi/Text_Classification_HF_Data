# -*- coding: utf-8 -*-
"""S_M_Ali_Zaidi_Assignment_P2_Loading_Pretrained_Model_and_tested_on_test_Set.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bpsbt3KVpsLJ2clLNjZTRJ7MSStHJsWR

S M Ali Zaidi, Cohort 4 Violet

Loading the Pretrained Model to Test and Deploy
"""

from itertools import chain

import matplotlib.pyplot as plt

import nltk
nltk.download('stopwords', quiet=True)
# nltk.download('punkt', quiet=True)
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

import numpy as np
import pandas as pd
import pickle
import seaborn as sns

from sklearn import feature_extraction
from sklearn.metrics import confusion_matrix
from sklearn import model_selection as ms
from sklearn import naive_bayes
from sklearn import preprocessing
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

import string

!pip install datasets

from datasets import load_dataset

# Choose a configuration: sentences_50agree, sentences_66agree, sentences_75agree, or sentences_allagree
config_name = 'sentences_50agree'  # As an example

dataset = load_dataset("financial_phrasebank", config_name)

# Now you can work with the dataset
print(dataset)

from datasets import load_dataset

# Load the dataset with a specific configuration
dataset = load_dataset("financial_phrasebank", "sentences_allagree")

# Access a specific split of the dataset
data_split = dataset["train"]

# View the first 5 entries
for i in range(5):
    print(data_split[i])

#printing 1st and 2nd row of the data set
dataset['train']['sentence'][0]
#dataset['train']['sentence'][1]

data_split

#prinitnig 1st 10 rows of the dataset
for i in range(10):
  print(dataset['train']['sentence'][i]," label:" ,dataset['train']['label'][i])

"""Converting the Dataset in Pandas dataframe

"""

import pandas as pd

# Convert the dataset to a pandas DataFrame
df = pd.DataFrame(dataset['train'])

df.head(15)

df.tail(15)

"""Unique categories:"""

categories = list(df['label'].unique())
categories

"""o Negative
1 Neutral
2 Positive
"""

df.shape

"""Unique Sentences"""

cleaned_sentences = list(df['sentence'].unique())
len(cleaned_sentences)

"""2259 unique sentences"""

df['label'].value_counts()

"""Number of unique rows by category"""

cat_unique_val = {}

for category in categories:
    mask = df['label'] == category
    list_length = len(list(df[mask]['sentence'].unique()))
    cat_unique_val.update({category: list_length})

    cat_unique_val

cat_unique_val

"""Dropping duplicate rows from each category"""

text_df = df.drop_duplicates()

text_df.shape

"""Text Cleaning

Removing stopwords and punctuation from "sentence" column
"""

stop_words = set(stopwords.words('english'))

regular_punct = list(string.punctuation)

text_df['sentence']

def text_preprocessing(x):
    filtered_sentence = []
    word_tokens = word_tokenize(x)

    for w in word_tokens:
        if w not in chain(stop_words, regular_punct):
            # we make sure that all words are written in lowercase
            filtered_sentence.append(w.lower())

    # Converting a list of strings back to a string
    filtered_sentence = " ".join(filtered_sentence)
    return filtered_sentence

import nltk
nltk.download('punkt')

text_df['sentence'] = text_df['sentence'].apply(text_preprocessing)

text_df['sentence']

"""Preprocessing the data"""

counts = feature_extraction.text.CountVectorizer()
X = counts.fit_transform(text_df['sentence'].values)
X.shape

y = text_df['label'].values

"""Defining the test set by defining trainig test and validation sets once again."""

# Spliting the data into training and a temp test set (80% train, 20% temp test)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)

# Spliting the temp test set into actual test and validation sets (50% test, 50% validation from the temp test)
# This results in 80% train, 10% validation, and 10% test set, assuming the initial split was 80-20.
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

"""Loading Pretrained Model"""

pickled_model = pickle.load(open('/content/model_save_fin.pkl', 'rb'))

pickled_model.score(X_train, y_train)

pickled_model.score(X_test, y_test)

y_pred = pickled_model.predict(X_test)
confusion_matrix(y_test, y_pred)

X_test[0]

# cat_unique_val is a list of integers or numpy.int64 values
target_names = [str(label) for label in cat_unique_val]

print(classification_report(y_test, y_pred, target_names=target_names))

mat = confusion_matrix(y_test, pickled_model.predict(X_test))
plt.figure(figsize=(18,12))
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,
            xticklabels=cat_unique_val,
            yticklabels=cat_unique_val
           )
plt.xlabel('true label')
plt.ylabel('predicted label');

"""Deploying the Model on GRADIO"""

!pip install gradio

import gradio as gr

"""Deploying the Model Using Pipleline by Transformers"""

from transformers import pipeline

# Load sentiment analysis model
sentiment_classifier = pipeline("sentiment-analysis")

def classify_sentiment(input_text):
    result = sentiment_classifier(input_text)
    return result[0]['label']

iface = gr.Interface(fn=classify_sentiment, inputs="text", outputs="text", title="Sentiment Analysis")
iface.launch()

"""Hugging Face"""

# When prompted for a password, use an access token with write permissions.
# Generate one from your settings:
# huggingface.co/settings/tokens

!git clone https://huggingface.co/spaces/AliXaidi/Sentiment_analysis

from transformers import pipeline

# Load sentiment analysis model
sentiment_classifier = pipeline("sentiment-analysis")

def classify_sentiment(input_text):
    result = sentiment_classifier(input_text)
    return result[0]['label']

iface = gr.Interface(fn=classify_sentiment, inputs="text", outputs="text", title="Sentiment Analysis")
iface.launch()

git add app.py
git commit -m "Add application file"
git push